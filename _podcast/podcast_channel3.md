---
layout: post
title: "实证研究数据处理的过程标准化探索"
---

## 写在前面

这个频道的出发点是，记录下我处理GLOC_24眼动实验数据的全过程。通过观察、分解数据处理、建模的全过程，描绘出一个可拓展到同类实证研究的R代码框架。概览的思路是：

1. 数据导入、清洗 --> 
2. 筛选出需要的列数据，整理排列为标签完整的行、列数据 --> 
3. 针对各个研究问题，存储各自所需数据的完整数据框 -->
4. 根据自变量、因变量的数据性质，划分几种主流的线性模型构建方法。每个方法包括数据的描述性统计、分布分析、固定和随机效应变量确认、初步拟合、拟合优度检验、数据变换、再次拟合、对比拟合优度、输出较优模型的统计结果。-->
5. 制图，使用 ggplot，根据数据特性，制作有区分度的图表。后期调整重点是比例、坐标轴值域、配色外观等。-->
6. 统计结果和制图的公式保存、结果输出，且全流程保存为 rmd 文档，写明过程，以便回溯。

这个概览的思路是和以前实验数据处理一脉相承的事情，虽然繁琐，但是并不陌生。然而这次想做的过程标准化探索，是借助AI大模型的力量，对已有的流程再做一次结构化处理。思路1：以工作流为基础，构建出帮助处理数据的智能体，以AI agent 为重要辅助，核心是搭建工作流和调用 LLM；思路2：以数据处理的逻辑块为基础，构建出半固定的流程界面，将AI 辅助固定在具体的任务上，目标产品是可视化的交互式网站界面，用户可以直接上传所有实验数据，在流程的引导下，半自动地描述数据、选择处理选项，完成全流程。

思路2 是我更想做到的，一个类似于“傻瓜式”的一站式数据处理网站，极大地有利于翻译实证研究者快速得出实验结果。这种形式的前驱是 Michael 那一套 Translog 数据放在 YAWAT 平台上一键导出的效果，但是 YAWAT 的数据是直接从 Translog 导出的，因此数据是从闭源到开源。而思路2想做的是从半开源（需要符合一定的数据特性，再加以描述）到闭源的效果。Translog 和 YAWAT 的缺点很明显，从数据报错到手工对齐的繁琐，再到导出的半原始数据，都造成了很多的不方便。我想做的新工具不见得能实际上产生更好的效果，但是，试一试吧，谁知道呢？